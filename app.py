import json
import pandas as pd
import streamlit as st
from urllib.parse import urlparse
import time
import requests

st.set_page_config(page_title="JP Rank Checker", page_icon="ğŸ”", layout="centered")
st.title("ğŸ” æ—¥æœ¬å‘ã‘ã‚·ãƒ³ãƒ—ãƒ« ãƒ©ãƒ³ã‚¯ãƒã‚§ãƒƒã‚«ãƒ¼")

st.caption("å›½ã¯å¸¸ã« JPï¼ˆæ—¥æœ¬ï¼‰ã€‚éƒ½å¸‚ã¯æœªæŒ‡å®šã§ã‚‚å¯ã€‚Googleã®ä¸Šä½çµæœå†…ã§ç‰¹å®šãƒ‰ãƒ¡ã‚¤ãƒ³ãŒä½•ä½ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚")
st.info("Secretsã« SERPAPI_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚Streamlit Cloud > App > Settings > Secrets")

API_URL = "https://serpapi.com/search.json"

def _domain_of(s: str) -> str:
    if not s: return ""
    if "://" in s:
        return urlparse(s).netloc.lower().lstrip(".")
    return s.lower().lstrip(".")

def fetch_serp(keyword: str, api_key: str, city: str|None, device: str, num: int):
    if not api_key:
        raise RuntimeError("SERPAPI_KEY ãŒæœªè¨­å®šã§ã™ã€‚Secretsã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
    params = {
        "engine": "google",
        "q": keyword,
        "gl": "JP",      # æ—¥æœ¬å›ºå®š
        "hl": "ja",
        "device": device,
        "num": num,
        "api_key": api_key
    }
    if city:
        params["location"] = city
    # ãƒªãƒˆãƒ©ã‚¤
    last_status = None
    last_text = ""
    for attempt in range(5):
        r = requests.get(API_URL, params=params, timeout=30)
        if r.status_code == 200:
            return r.json()
        last_status = r.status_code
        last_text = r.text[:300]
        if r.status_code in (429, 500, 502, 503):
            time.sleep(1.5 * (attempt + 1))
            continue
        r.raise_for_status()
    raise RuntimeError(f"SERPå–å¾—å¤±æ•— status={last_status} body={last_text}")

def parse_rank(serp_json: dict, target_domain: str, top_k: int = 10):
    organic = (serp_json.get("organic_results") or [])[:top_k]
    rank, matched_url = None, ""
    for i, item in enumerate(organic, start=1):
        link = item.get("link") or item.get("url") or ""
        if target_domain and target_domain in link:
            rank, matched_url = i, link
            break
    features = {
        "featured_snippet": bool(serp_json.get("answer_box")),
        "paa": bool(serp_json.get("related_questions")),
        "video": any(("video" in str(item)) for item in organic),
        "local_pack": bool(serp_json.get("local_results")),
    }
    top = [{"rank": i, "title": item.get("title"), "url": item.get("link") or item.get("url")}
           for i, item in enumerate(organic, start=1)]
    return rank, matched_url, features, top

with st.form("single"):
    kw = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", placeholder="ä¾‹ï¼‰ãƒ©ãƒ³ã‚¯ãƒˆãƒ©ãƒƒã‚«ãƒ¼")
    target = st.text_input("å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³ / URL", placeholder="ä¾‹ï¼‰example.com ã¾ãŸã¯ https://example.com")
    col1, col2 = st.columns(2)
    with col1:
        device = st.selectbox("ãƒ‡ãƒã‚¤ã‚¹", ["desktop", "mobile"], index=0)
    with col2:
        city = st.text_input("éƒ½å¸‚ï¼ˆä»»æ„ï¼‰", placeholder="ä¾‹ï¼‰Tokyo / Osakaï¼ˆç©ºæ¬„OKï¼‰")
    topk = st.slider("ä¸Šä½ä½•ä½ã¾ã§å–å¾—", min_value=10, max_value=100, value=10, step=10)
    submitted = st.form_submit_button("ãƒã‚§ãƒƒã‚¯ã™ã‚‹")

if submitted:
    try:
        api_key = st.secrets.get("SERPAPI_KEY")
        serp = fetch_serp(kw, api_key=api_key, city=(city or None), device=device, num=topk)
        rank, url, feats, top = parse_rank(serp, _domain_of(target), top_k=topk)

        st.subheader("çµæœ")
        st.write(f"**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**ï¼š{kw}")
        st.write(f"**å¯¾è±¡**ï¼š{_domain_of(target)}")
        if rank is None:
            st.error(f"ä¸Šä½{topk}ä½ã«å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆåœå¤–ï¼‰ã€‚")
        else:
            st.success(f"âœ… é †ä½ï¼š**{rank} ä½**")
            st.write(f"ä¸€è‡´URLï¼š{url}")

        with st.expander("SERPç‰¹å¾´"):
            st.json(feats)

        df = pd.DataFrame(top, columns=["rank", "title", "url"])
        st.dataframe(df, use_container_width=True)
        st.download_button("ä¸Šä½çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", df.to_csv(index=False), "top_results.csv", "text/csv")

    except Exception as e:
        st.exception(e)
